===========================
DIPENDENZE PYTHON
===========================
 - pandas (dovrebbe essere inclusa con flexmatcher)
 - xlrd (per i file xls)
 - flexmatcher (custom)
 - valentine

===========================
FONTI SCOMODE
===========================
FR_sole24ore -> non ha i nomi delle colonne, tocca aggiungerli a mano (risolvibile comunque)
valueToday_dataset -> è scomoda in quanto ci sono campi con liste, conviene cambiarla

===========================
HISTORY
===========================
script che converte tutti i csv/xml in json per uniformità (da anche una sistemata ai json presenti 
per migliore leggibilità) (script: filesConverter.py)

MEDIATED SCHEMA:
 > chatgpt
   - prompt per fare lo schema mediato tramite chatgpt
   - buoni risultati, fare test piu ciotti?
   - BISOGNA METTERE I PROMP E I RESULT IN DEI TXT
 > flexmatcher
   - libreria flexmatcher ha roba deprecata non si puo usare
   - risolte a mano le robe deprecate
   - tempo addestramento sul tostapane mio: 4 min e mezzo (/flexmatcher-ms/main.py)
   - da risultati discreti, fare test piu ciotti?
   - non tenendo in considerazione i nomi delle colonne non riesce ovviemente a cogliere differenze tra
     ad esempio revenue2017 e revenue2020
   - richiede di fare il labeling prima di fare i match, e quindi fondamentalmente richiede di avere gia uno schema mediato
   - durante il training da spesso dei warning dicendo che non raggiunge la piena convergenza l'algoritmo di scikitlearn
     usato dalla libreria, per averla toccherebbe mettere le mani in pasta ed aumentare le iterazioni massime (rischiando 
     overfitting), un po' una palla
 > valentine (con coma, provare bene anche altri matcher?)
   - a fare i comparisons (192) ci mette l'ira di dio, tempo tot: 32min (sempre sul tostapane)
   - parallelizzando sto scempio (ci mette circa 20 minuti ora, collo di bottiglia non sono le cpu ma la ram 
     disponibile -> ergo sul pc mio posso usare solo 2 sub-processi)
   - creato script che processa e insieme i match tra tutte le coppie possibili e crea lo schema mediato (processMatches.py)
   - il valore di tuning della soglia di confidenza che sembra comportarsi meglio è 0.3 (forse provare un pochino 
     piu alta, con 0.3 mette in comune industry e country)
     0.33 si comporta meglio
   - lo schema mediato è un json: mediated-schema.json
   - non riesce a cogliere differenze tra ad esempio revenue2017 e revenue2020 nonostante dovrebbe tenere in considerazione
     anche i nomi delle colonne oltre che i valori (aumentare troppo la soglia per provare a risolvere ciò però fa
     trovare troppi pochi match, quindi diventa necessario piu lavoro manuale)
  > creato script per fare il mega tabellone dato lo schema mediato (makeTable.py)

PROCESSING:
 > country
   - prendendo l'ultima parola (separando con le virgole la stringa) di country essa corrisponderà spesso all'effettiva nazione
   - utilizzata country_converter per uniformare la nomenclatura delle nazioni
   - qualora l'ultima parola non è una nazione, con ottima probabilità è invece una città, utilizzando una libreria che trova
     le nazioni dalle città (geopy.geocoders Nominatim)
   - il nome della nazione è in lingua originale
   - utilizziamo la libreria "translators" con bing per fare le traduzioni
   - tutte le richieste delle città vengono cachate in un dizionario in quanto farle è abbastanza lento (risparmio di tempo circa 50%)
   - dovrebbe poter essere anche parallelizzabile, ancora non effettuata parallelizzazione
 > founding_year
   - viene estratto l'anno con un approccio rule-based basato sui datasets
   - viene prima controllata la presenza del campo founding_year, qualora non fosse presente si utilizza founding_date
   - ci sono molti elementi senza una data di fondazione (ca 44k su 65k)

BLOCKING:
  > country
   - blocking con match esatto su country
  > company_name
   - blocking sulle prime due lettere di company_name

PAIRWISE MATCHING:
  > levenshtein distance su company_name
    - algoritmo implementato in c in quanto in python era troppo proibitivo lavorare con blocchi grandi
    - la distanza di levenshtein è pesata: INSERTION_COST 0.4, DELETION_COST 0.4, SUBSTITUTION_COST 2
    - la distanza di levenshtein è effettuata su stringe private di tutti i caratteri non alfanumerici
    - soglia di distanza massima per considerare match: 0.13
    - utilizzata libreria cJSON per parsare i json
    - speedup incredibile rispetto a python
